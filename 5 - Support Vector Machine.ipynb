{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb588965-29b8-4133-bb81-6a36e06b68fb",
   "metadata": {},
   "source": [
    "### Learning Guide\n",
    "1. What is Support Vector Machine?\n",
    "2. Why Support Vector Machine?\n",
    "3. Understanding Support Vector Machine\n",
    "4. Advantages and Disadvantages of Support Vector Machine\n",
    "5. Use Case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d2dcf3-4352-4927-8537-75bcd791296e",
   "metadata": {},
   "source": [
    "## What is Support Vector Machine?\n",
    "Support Vector Machine (SVM) is a powerful supervised machine learning algorithm used for both classification and regression tasks. It's particularly well-suited for classification of complex datasets with a clear margin of separation.\r\n",
    "\r\n",
    "### How Support Vector Machine Works:\r\n",
    "\r\n",
    "1. **Margin Maximization**:\r\n",
    "   - SVM aims to find the hyperplane that maximizes the margin between classes. The hyperplane is the decision boundary that separates the data points of different classes with the largest possible margin.\r\n",
    "   \r\n",
    "2. **Support Vectors**:\r\n",
    "   - Support vectors are the data points closest to the hyperplane. These points determine the position and orientation of the hyperplane. Only the support vectors have an influence on the location of the hyperplane.\r\n",
    "\r\n",
    "3. **Kernel Trick**:\r\n",
    "   - SVM can efficiently handle non-linear decision boundaries through the use of kernel functions. Kernel functions transform the input space into a higher-dimensional space where the classes are linearly separable. Common kernel functions include Linear, Polynomial, Gaussian (RBF), and Sigmoid kernels.\r\n",
    "\r\n",
    "### Key Concepts:\r\n",
    "\r\n",
    "- **C Parameter**:\r\n",
    "  - C parameter controls the trade-off between maximizing the margin and minimizing the classification error. A smaller C value leads to a larger margin but may misclassify some training examples, while a larger C value allows more classification errors but may lead to a narrower margin.\r\n",
    "\r\n",
    "- **Kernel Trick**:\r\n",
    "  - SVM uses kernel functions to map input features into higher-dimensional space, where classes may be more easily separable.\r\n",
    "\r\n",
    "- **Soft Margin SVM**:\r\n",
    "  - Soft Margin SVM allows for misclassification of some training examples to achieve a more generalized decision boundary. It introduces a penalty term for misigh accuracy and robust performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e57c57-f510-41e1-ba5a-952a284680b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a58792-c98d-45e9-9e93-d764f843b7d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1edb00d-84bb-4a2d-9ab5-3d55cc4eb4d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d24de854-f27d-46c4-ad0f-61ff1b69a8e6",
   "metadata": {},
   "source": [
    "### Advantages of Support Vector Machine:\n",
    "\n",
    "- **Effective in High-Dimensional Spaces**: SVM is effective even in high-dimensional spaces, making it suitable for datasets with many features.\n",
    "- **Robust to Overfitting**: SVM can avoid overfitting by maximizing the margin and penalizing misclassifications.\n",
    "- **Versatile**: SVM supports various kernel functions, allowing it to handle non-linear decision boundaries.\n",
    "\n",
    "### Disadvantages of Support Vector Machine:\n",
    "\n",
    "- **Computational Complexity**: Training SVMs can be computationally intensive, especially for large datasets.\n",
    "- **Sensitivity to Noise**: SVMs are sensitive to noise and outliers in the dataset, which can affect the positioning of the hyperplane.\n",
    "- **Choice of Kernel**: The performance of SVMs heavily depends on the choice of kernel function and its parameters. Selecting the appropriate kernel function requires domain knowledge and experimentation.\n",
    "\n",
    "In summary, Support Vector Machine is a versatile and powerful algorithm for classification tasks, capable of handling complex datasets with non-linear decision boundaries. With proper parameter tuning and kernel selection, SVMs can achieve high accuracy and robust performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f91a3f-1105-4f40-9611-70434a7e4522",
   "metadata": {},
   "source": [
    "## Use Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b0aa73-7d6a-4764-b566-f95b7383527e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "d = make_blobs()\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a46ed1-90d7-4531-9afe-85975509c90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create 40 seperable points\n",
    "X, y = make_blobs(n_samples=40, centers=2, random_state=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5965572-62fb-4c62-b42b-0bb8fd1e5540",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fbc1c3-8d87-4041-ada8-6df7531f9573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit SVM model\n",
    "clf = svm.SVC(kernel='linear', C=1)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b871503f-76df-45ca-86cc-5519b217e59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot data points and decision boundary\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=plt.cm.Paired)\n",
    "ax = plt.gca()\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "\n",
    "# Create grid to evaluate model\n",
    "xx = np.linspace(xlim[0], xlim[1], 30)\n",
    "yy = np.linspace(ylim[0], ylim[1], 30)\n",
    "YY, XX = np.meshgrid(yy, xx)\n",
    "xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
    "Z = clf.decision_function(xy).reshape(XX.shape)\n",
    "\n",
    "# Plot decision boundary and margins\n",
    "ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5,\n",
    "           linestyles=['--', '-', '--'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e64b9dc-0305-423c-814f-37ef00e79307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "newData = [[3,4],[5,6]]\n",
    "clf.predict(newData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2dbd91-d3de-4d3a-a512-a5a5bacdd09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "ax = plt.gca()\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "\n",
    "# Create grid to evaluate model\n",
    "xx = np.linspace(xlim[0], xlim[1], 30)\n",
    "yy = np.linspace(ylim[0], ylim[1], 30)\n",
    "YY, XX = np.meshgrid(yy, xx)\n",
    "xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
    "Z = clf.decision_function(xy).reshape(XX.shape)\n",
    "\n",
    "# Plot decision boundary and margins\n",
    "ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5,\n",
    "           linestyles=['--', '-', '--'])\n",
    "ax.scatter(clf.support_vectors_[:,0],\n",
    "          clf.support_vectors_[:,1], s=100,\n",
    "          linewidth=1, facecolors='none')\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230c6888-b4ad-4f3a-a885-6654281bdbed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eebf324-9a70-43b5-8eb2-4286b209629a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "X, y = make_blobs(n_samples=100, centers=2, random_state=0, cluster_std=0.60)\n",
    "\n",
    "# Fit SVM model\n",
    "clf = svm.SVC(kernel='linear', C=1000)\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Plot data points and decision boundary\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=plt.cm.Paired)\n",
    "ax = plt.gca()\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "\n",
    "# Create grid to evaluate model\n",
    "xx = np.linspace(xlim[0], xlim[1], 30)\n",
    "yy = np.linspace(ylim[0], ylim[1], 30)\n",
    "YY, XX = np.meshgrid(yy, xx)\n",
    "xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
    "Z = clf.decision_function(xy).reshape(XX.shape)\n",
    "\n",
    "# Plot decision boundary and margins\n",
    "ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5,\n",
    "           linestyles=['--', '-', '--'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226df7ce-8d06-43eb-b130-1943e6e905b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
